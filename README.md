# 🧠 Large Language Model(LLM) 기초 정리
> 본 문서는 LLM의 개념, 동작 방식, 주요 용어, 학습 구조 등에 대한 기초 내용을 정리한 자료입니다. AI 모델 개발 및 적용을 위한 기본 지식 습득을 목적으로 합니다.

---

## 📌 1.LLM 이란?
- **Large Language Model**은 대규모 텍스트 데이터를 기반으로 학습한 자연어 처리 모델 입니다.
- 인간 언어의 패턴을 학습하여, 텍스트 생성, 요약, 번역, 질문응답, 코드 생성 등 다양한 작업 수행이 가능합니다.
- 대표 예시 : GPT, BERT, LLaMA, Claude, Mistral, PaLM 등

---

## ⚙️ 2. LLM의 동작 구조
1. **토큰화(Tokenization)**
   - 입력 문장을 단어/부분단어 단위의 토큰으로 분해
3. **임베딩(Embedding)**
   - 토큰을 고차원 벡터로 변환
5. **Transformer 처리**
   - Self-Attention을 활용하여 문맥 정보 이해
6. **출력 생성**
   - 다음 단어를 예측하고 출력
  
---

## 🧠 3. 학습 방식
| 방식 | 설명 |
| Pre-training | 웹/문서/뉴스 등 대규모 텍스트로 언어 패턴 학습 |
| Fine-tuning | 특정 분야 데이터로 추가 학습 (예: 의학, 법률 등) |
| PEFT/LoRA | 파라미터 효율적 조정 방식, 전체 모델 재학습 불필요 |

---

| 용어 | 의미 |
|------|------|
| Token | 문장을 구성하는 최소 단위 (단어 또는 부분 단어) |
| Attention | 중요한 단어에 집중하여 문맥 파악 |
| Transformer | LLM의 기본 구조, Self-Attention 기반 |
| Parameter | 모델의 가중치, 수가 많을수록 복잡한 패턴 표현 가능 |
| Context Window | 한 번에 처리 가능한 토큰 수 |
| Temperature | 생성되는 문장의 다양성 조절 값 (낮을수록 정답에 수렴) |

---

## 📈 5. 대표 LLM 비교

| 모델명 | 규모 | 소속 | 오픈소스 | 특징 |
|--------|------|------|-----------|------|
| GPT-3.5 / GPT-4 | 175B+ | OpenAI | ❌ | 상업용 중심 |
| LLaMA 2 | 7B~65B | Meta | ✅ | 연구용 허용 |
| Mistral | 7B | Mistral AI | ✅ | 경량이지만 높은 성능 |
| Claude | 100B+ | Anthropic | ❌ | 안전성 중심 |
| Phi-2 | 2.7B | Microsoft | ✅ | 소형임에도 성능 우수 |

---

## 💡 6. 활용 예시

- AI 챗봇 (예: ChatGPT)
- 텍스트 요약 / 분류 / 생성
- 자동 번역
- 코드 보완 및 생성 (Codex, Starcoder 등)
- 질의응답 시스템 (QA)

---



